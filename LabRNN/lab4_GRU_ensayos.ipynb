{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASFLzUqo38zD"
      },
      "source": [
        "# Laboratorio 4\n",
        "\n",
        "Autores:\n",
        "\n",
        "* Kuntur Muenala\n",
        "* Diego Villacreses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUsIkBICAUu9",
        "outputId": "7089422b-0cb6-48e8-cffe-14eec41307a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "os.getcwd()='/home/kmuenala/nlp/data'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import spacy\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "#from skopt import BayesSearchCV\n",
        "#from skopt.space import Real, Integer\n",
        "  # Intento fallido de optimización bayesiana sobre hyperparámetros de Naive Bayes\n",
        "\n",
        "import time\n",
        "import seaborn as sns\n",
        "\n",
        "os.chdir(\"/home/kmuenala/nlp/data\")\n",
        "print(f\"{os.getcwd()=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc9zHV5jAUvO"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "Dataset:\n",
        "* https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TP_1J4YXAUvV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f80a71e-446c-40f0-f461-501f4460d6f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator, GloVe\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device= 'cpu'\n",
        "\n",
        "# Tokenizer\n",
        "# tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
        "# hf_token= userdata.get('HF_TOKEN')\n",
        "\n",
        "# reader_model_name = \"meta-llama/Meta-Llama-3.1-8B\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(reader_model_name)\n",
        "\n",
        "# Function to yield tokens from phrases for vocabulary building\n",
        "def yield_tokens(phrases):\n",
        "    for phrase in phrases:\n",
        "        yield word_tokenize(phrase)\n",
        "\n",
        "# Custom dataset class\n",
        "class MovieReviewsDataset(Dataset):\n",
        "    def __init__(self, phrases, sentiments, vocab, max_len=100):\n",
        "        self.phrases = phrases\n",
        "        self.sentiments = sentiments\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.phrases)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        phrase = self.phrases[idx]\n",
        "        sentiment = self.sentiments[idx]\n",
        "        tokens = word_tokenize(phrase)\n",
        "        # tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
        "        # tokens = [stemmer.stem(word) for word in tokens]  # Apply stemming\n",
        "        # tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Apply lemmatization\n",
        "        token_ids = [self.vocab[token] if token in self.vocab else self.vocab[\"<unk>\"] for token in tokens]\n",
        "        # Padding or truncation\n",
        "        if len(token_ids) > self.max_len:\n",
        "            token_ids = token_ids[:self.max_len]\n",
        "        else:\n",
        "            token_ids += [self.vocab[\"<pad>\"]] * (self.max_len - len(token_ids))\n",
        "\n",
        "        return torch.tensor(token_ids), torch.tensor(sentiment, dtype=torch.long)\n",
        "\n",
        "# Preprocessing the data\n",
        "def preprocess_data(file_path):\n",
        "    df = pd.read_csv(file_path, delimiter='\\t')\n",
        "\n",
        "    df = df[['Phrase', 'Sentiment']]\n",
        "    recode_dict = {0:1,1:1,2:2,3:3,4:3}\n",
        "    df = df.replace({'Sentiment': recode_dict})\n",
        "\n",
        "    # df['Phrase'] = df['Phrase'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s.,]', '', x))\n",
        "    # df['Phrase'] = df['Phrase'].apply(lambda x: x.lower())\n",
        "    phrases = df['Phrase'].values\n",
        "    sentiments = df['Sentiment'].values\n",
        "\n",
        "    return phrases, sentiments\n",
        "\n",
        "# Load data\n",
        "phrases, sentiments = preprocess_data('train.tsv')\n",
        "\n",
        "# Split the data\n",
        "train_phrases, valid_phrases, train_sentiments, valid_sentiments = train_test_split(phrases, sentiments, test_size=0.2, random_state=13, stratify=sentiments)\n",
        "\n",
        "# Build the vocabulary\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_phrases), specials=[\"<unk>\", \"<pad>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "# Load GloVe vectors\n",
        "glove_vectors = GloVe(name=\"6B\", dim=100)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = MovieReviewsDataset(train_phrases, train_sentiments, vocab)\n",
        "valid_dataset = MovieReviewsDataset(valid_phrases, valid_sentiments, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zsb9Z5Op7Hb",
        "outputId": "3061f8ee-f8ac-4412-fa8f-482b568ad6f3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Vocab()"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(train_dataset.sentiments).value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "gCr26AbmObpb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55a1d441-caec-4a96-b71d-9800709b4954"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    0.509948\n",
              "3    0.269976\n",
              "1    0.220076\n",
              "Name: proportion, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(valid_dataset.sentiments).value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "U0LE-Lu9OcT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea2f2e04-b431-4031-f5de-7ac3962cf35d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    0.509932\n",
              "3    0.269992\n",
              "1    0.220076\n",
              "Name: proportion, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tllk8NbHAUva"
      },
      "source": [
        "## GRU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentGRU(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n",
        "        super(SentimentGRU, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=n_layers,\n",
        "                          bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        if self.gru.bidirectional:\n",
        "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "        else:\n",
        "            hidden = hidden[-1,:,:]\n",
        "\n",
        "        hidden = self.dropout(hidden)\n",
        "        return self.fc(hidden)"
      ],
      "metadata": {
        "id": "Z43eA4pKP219"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9uIsGa6wAUvd"
      },
      "outputs": [],
      "source": [
        "# Accuracy calculation function\n",
        "def multi_class_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch.\n",
        "    Get the class with the highest probability and compare it with the true class.\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim=1, keepdim=True)  # get the index of the max probability\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    return correct.sum().float() / torch.FloatTensor([y.shape[0]]).to(y.device)\n",
        "\n",
        "# Training function\n",
        "def train(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    for text, sentiment in loader:\n",
        "        text, sentiment = text.to(device), sentiment.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(text)\n",
        "        loss = criterion(predictions, sentiment)\n",
        "        acc = multi_class_accuracy(predictions, sentiment)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(loader), epoch_acc / len(loader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    all_preds = []\n",
        "    with torch.no_grad():\n",
        "        for text, sentiment in loader:\n",
        "            text, sentiment = text.to(device), sentiment.to(device)\n",
        "            predictions = model(text)\n",
        "            loss = criterion(predictions, sentiment)\n",
        "            acc = multi_class_accuracy(predictions, sentiment)\n",
        "            y_hat = predictions.argmax(dim=1, keepdim=True)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "            all_preds.append(y_hat)\n",
        "\n",
        "    return epoch_loss / len(loader), epoch_acc / len(loader), all_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ODhPPYc6AUvi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cd5d814-1325-4dc0-df6e-dbd6dc2e0d58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Train Loss: 0.764 | Train Acc: 66.69%\n",
            "Valid Loss: 0.644 | Valid Acc: 72.53% | Valid F1-Weighted: 71.76%\n",
            "Epoch 2\n",
            "Train Loss: 0.650 | Train Acc: 72.57%\n",
            "Valid Loss: 0.599 | Valid Acc: 75.45% | Valid F1-Weighted: 75.23%\n",
            "Epoch 3\n",
            "Train Loss: 0.618 | Train Acc: 74.05%\n",
            "Valid Loss: 0.586 | Valid Acc: 75.86% | Valid F1-Weighted: 75.85%\n",
            "Epoch 4\n",
            "Train Loss: 0.574 | Train Acc: 76.13%\n",
            "Valid Loss: 0.575 | Valid Acc: 76.40% | Valid F1-Weighted: 76.37%\n",
            "Epoch 5\n",
            "Train Loss: 0.546 | Train Acc: 77.45%\n",
            "Valid Loss: 0.582 | Valid Acc: 76.55% | Valid F1-Weighted: 76.52%\n",
            "Epoch 6\n",
            "Train Loss: 0.525 | Train Acc: 78.24%\n",
            "Valid Loss: 0.576 | Valid Acc: 76.56% | Valid F1-Weighted: 76.55%\n",
            "Epoch 7\n",
            "Train Loss: 0.505 | Train Acc: 79.15%\n",
            "Valid Loss: 0.582 | Valid Acc: 76.83% | Valid F1-Weighted: 76.79%\n",
            "Epoch 8\n",
            "Train Loss: 0.489 | Train Acc: 79.79%\n",
            "Valid Loss: 0.583 | Valid Acc: 76.97% | Valid F1-Weighted: 76.92%\n",
            "Epoch 9\n",
            "Train Loss: 0.474 | Train Acc: 80.28%\n",
            "Valid Loss: 0.599 | Valid Acc: 76.70% | Valid F1-Weighted: 76.70%\n",
            "Epoch 10\n",
            "Train Loss: 0.464 | Train Acc: 80.77%\n",
            "Valid Loss: 0.604 | Valid Acc: 76.72% | Valid F1-Weighted: 76.72%\n"
          ]
        }
      ],
      "source": [
        "# Model hyperparameters\n",
        "# Initialize the model\n",
        "# Sin Remove stopwords\n",
        "# Sin Apply stemming\n",
        "# Sin Apply lemmatization\n",
        "INPUT_DIM = len(vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 5\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = vocab[\"<pad>\"]\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Model\n",
        "model = SentimentGRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX).to(device)\n",
        "\n",
        "glove_vectors = GloVe(name=\"6B\", dim=EMBEDDING_DIM)\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Use pretrained embeddings\n",
        "embedding_matrix = torch.zeros(INPUT_DIM, EMBEDDING_DIM)\n",
        "for i, token in enumerate(vocab.get_itos()):\n",
        "    if token in glove_vectors.stoi:\n",
        "        embedding_matrix[i] = glove_vectors[token]\n",
        "model.embedding.weight.data.copy_(embedding_matrix)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# Training loop\n",
        "N_EPOCHS = 10\n",
        "nn_res = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
        "    valid_loss, valid_acc,all_preds = evaluate(model, valid_loader, criterion)\n",
        "\n",
        "    y_hat_val = []\n",
        "    for n in range(len(all_preds)):\n",
        "      y_hat_val += [i[0] for i in all_preds[n].cpu().numpy()]\n",
        "\n",
        "    f1_w = f1_score(y_true=valid_dataset.sentiments,\n",
        "         y_pred=y_hat_val,\n",
        "         average='weighted')\n",
        "\n",
        "    print(f'Epoch {epoch+1}')\n",
        "    print(f'Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'Valid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:.2f}% | Valid F1-Weighted: {f1_w*100:.2f}%')\n",
        "\n",
        "    nn_res.append([epoch,train_loss,train_acc,valid_loss,valid_acc,f1_w])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model hyperparameters\n",
        "# Initialize the model\n",
        "# con Remove stopwords\n",
        "# con Apply stemming\n",
        "# con Apply lemmatization\n",
        "INPUT_DIM = len(vocab)\n",
        "EMBEDDING_DIM = 200\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 5\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = vocab[\"<pad>\"]\n",
        "BATCH_SIZE = 32\n",
        "N_EPOCHS = 10\n",
        "\n",
        "# Model\n",
        "model = SentimentGRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX).to(device)\n",
        "\n",
        "glove_vectors = GloVe(name=\"6B\", dim=EMBEDDING_DIM)\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Use pretrained embeddings\n",
        "embedding_matrix = torch.zeros(INPUT_DIM, EMBEDDING_DIM)\n",
        "for i, token in enumerate(vocab.get_itos()):\n",
        "    if token in glove_vectors.stoi:\n",
        "        embedding_matrix[i] = glove_vectors[token]\n",
        "model.embedding.weight.data.copy_(embedding_matrix)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "nn_res = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
        "    valid_loss, valid_acc,all_preds = evaluate(model, valid_loader, criterion)\n",
        "\n",
        "    y_hat_val = []\n",
        "    for n in range(len(all_preds)):\n",
        "      y_hat_val += [i[0] for i in all_preds[n].cpu().numpy()]\n",
        "\n",
        "    f1_w = f1_score(y_true=valid_dataset.sentiments,\n",
        "         y_pred=y_hat_val,\n",
        "         average='weighted')\n",
        "\n",
        "    print(f'Epoch {epoch+1}')\n",
        "    print(f'Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'Valid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:.2f}% | Valid F1-Weighted: {f1_w*100:.2f}%')\n",
        "\n",
        "    nn_res.append([epoch,train_loss,train_acc,valid_loss,valid_acc,f1_w])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY-M8umuSS3o",
        "outputId": "9d94dd4d-6ef9-4599-929d-b4500bd6eadd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Train Loss: 1.092 | Train Acc: 56.13%\n",
            "Valid Loss: 1.009 | Valid Acc: 59.48% | Valid F1-Weighted: 55.64%\n",
            "Epoch 2\n",
            "Train Loss: 1.021 | Train Acc: 58.88%\n",
            "Valid Loss: 0.981 | Valid Acc: 60.71% | Valid F1-Weighted: 57.73%\n",
            "Epoch 3\n",
            "Train Loss: 0.986 | Train Acc: 60.31%\n",
            "Valid Loss: 0.968 | Valid Acc: 61.27% | Valid F1-Weighted: 57.87%\n",
            "Epoch 4\n",
            "Train Loss: 0.962 | Train Acc: 61.46%\n",
            "Valid Loss: 0.957 | Valid Acc: 61.76% | Valid F1-Weighted: 59.10%\n",
            "Epoch 5\n",
            "Train Loss: 0.982 | Train Acc: 60.83%\n",
            "Valid Loss: 0.975 | Valid Acc: 61.00% | Valid F1-Weighted: 59.50%\n",
            "Epoch 6\n",
            "Train Loss: 0.969 | Train Acc: 61.36%\n",
            "Valid Loss: 0.969 | Valid Acc: 61.24% | Valid F1-Weighted: 59.23%\n",
            "Epoch 7\n",
            "Train Loss: 0.957 | Train Acc: 61.99%\n",
            "Valid Loss: 0.964 | Valid Acc: 61.60% | Valid F1-Weighted: 59.72%\n",
            "Epoch 8\n",
            "Train Loss: 0.950 | Train Acc: 62.20%\n",
            "Valid Loss: 0.962 | Valid Acc: 61.92% | Valid F1-Weighted: 59.98%\n",
            "Epoch 9\n",
            "Train Loss: 0.941 | Train Acc: 62.54%\n",
            "Valid Loss: 0.961 | Valid Acc: 61.71% | Valid F1-Weighted: 58.77%\n",
            "Epoch 10\n",
            "Train Loss: 0.935 | Train Acc: 62.89%\n",
            "Valid Loss: 0.965 | Valid Acc: 61.77% | Valid F1-Weighted: 59.30%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ925ZjiAUvk",
        "outputId": "bd88edc4-e93d-4ed0-a180-59b31232b2f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Train Loss: 1.115 | Train Acc: 55.42%\n",
            "Valid Loss: 1.028 | Valid Acc: 58.97% | Valid F1-Weighted: 56.62%\n",
            "Epoch 2\n",
            "Train Loss: 1.047 | Train Acc: 57.69%\n",
            "Valid Loss: 0.996 | Valid Acc: 60.06% | Valid F1-Weighted: 56.79%\n",
            "Epoch 3\n",
            "Train Loss: 1.017 | Train Acc: 59.23%\n",
            "Valid Loss: 0.984 | Valid Acc: 60.80% | Valid F1-Weighted: 59.05%\n",
            "Epoch 4\n",
            "Train Loss: 0.993 | Train Acc: 60.10%\n",
            "Valid Loss: 0.992 | Valid Acc: 60.80% | Valid F1-Weighted: 57.26%\n",
            "Epoch 5\n",
            "Train Loss: 0.978 | Train Acc: 60.83%\n",
            "Valid Loss: 0.966 | Valid Acc: 61.54% | Valid F1-Weighted: 58.37%\n",
            "Epoch 6\n",
            "Train Loss: 0.962 | Train Acc: 61.43%\n",
            "Valid Loss: 0.964 | Valid Acc: 61.46% | Valid F1-Weighted: 59.09%\n",
            "Epoch 7\n",
            "Train Loss: 0.953 | Train Acc: 61.75%\n",
            "Valid Loss: 0.974 | Valid Acc: 61.45% | Valid F1-Weighted: 58.17%\n",
            "Epoch 8\n",
            "Train Loss: 0.938 | Train Acc: 62.42%\n",
            "Valid Loss: 0.958 | Valid Acc: 61.84% | Valid F1-Weighted: 59.17%\n",
            "Epoch 9\n",
            "Train Loss: 0.931 | Train Acc: 62.67%\n",
            "Valid Loss: 0.953 | Valid Acc: 61.73% | Valid F1-Weighted: 58.70%\n",
            "Epoch 10\n",
            "Train Loss: 0.923 | Train Acc: 63.06%\n",
            "Valid Loss: 0.950 | Valid Acc: 62.32% | Valid F1-Weighted: 60.26%\n"
          ]
        }
      ],
      "source": [
        "# Model hyperparameters\n",
        "# Initialize the model\n",
        "INPUT_DIM = len(vocab)\n",
        "EMBEDDING_DIM = 200\n",
        "HIDDEN_DIM = 300\n",
        "OUTPUT_DIM = 5\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.6\n",
        "PAD_IDX = vocab[\"<pad>\"]\n",
        "BATCH_SIZE = 64\n",
        "N_EPOCHS = 10\n",
        "\n",
        "# Model\n",
        "model = SentimentGRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX).to(device)\n",
        "\n",
        "glove_vectors = GloVe(name=\"6B\", dim=EMBEDDING_DIM)\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Use pretrained embeddings\n",
        "embedding_matrix = torch.zeros(INPUT_DIM, EMBEDDING_DIM)\n",
        "for i, token in enumerate(vocab.get_itos()):\n",
        "    if token in glove_vectors.stoi:\n",
        "        embedding_matrix[i] = glove_vectors[token]\n",
        "model.embedding.weight.data.copy_(embedding_matrix)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# Training loop\n",
        "nn_res = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
        "    valid_loss, valid_acc,all_preds = evaluate(model, valid_loader, criterion)\n",
        "\n",
        "    y_hat_val = []\n",
        "    for n in range(len(all_preds)):\n",
        "      y_hat_val += [i[0] for i in all_preds[n].cpu().numpy()]\n",
        "\n",
        "    f1_w = f1_score(y_true=valid_dataset.sentiments,\n",
        "         y_pred=y_hat_val,\n",
        "         average='weighted')\n",
        "\n",
        "    print(f'Epoch {epoch+1}')\n",
        "    print(f'Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'Valid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:.2f}% | Valid F1-Weighted: {f1_w*100:.2f}%')\n",
        "\n",
        "    nn_res.append([epoch,train_loss,train_acc,valid_loss,valid_acc,f1_w])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_cAm95UAUvn",
        "outputId": "b0ec6d4f-8209-41f7-e2f4-65e017a0fcae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Train Loss: 1.267 | Train Acc: 51.11%\n",
            "Valid Loss: 1.168 | Valid Acc: 53.22% | Valid F1-Weighted: 44.52%\n",
            "Epoch 2\n",
            "Train Loss: 1.143 | Train Acc: 54.63%\n",
            "Valid Loss: 1.045 | Valid Acc: 58.43% | Valid F1-Weighted: 53.02%\n",
            "Epoch 3\n",
            "Train Loss: 1.053 | Train Acc: 57.98%\n",
            "Valid Loss: 1.008 | Valid Acc: 59.64% | Valid F1-Weighted: 55.61%\n",
            "Epoch 4\n",
            "Train Loss: 1.019 | Train Acc: 59.09%\n",
            "Valid Loss: 0.992 | Valid Acc: 60.06% | Valid F1-Weighted: 55.84%\n",
            "Epoch 5\n",
            "Train Loss: 0.996 | Train Acc: 59.99%\n",
            "Valid Loss: 0.985 | Valid Acc: 60.55% | Valid F1-Weighted: 58.17%\n",
            "Epoch 6\n",
            "Train Loss: 0.979 | Train Acc: 60.55%\n",
            "Valid Loss: 0.972 | Valid Acc: 61.10% | Valid F1-Weighted: 58.66%\n",
            "Epoch 7\n",
            "Train Loss: 0.964 | Train Acc: 61.23%\n",
            "Valid Loss: 0.969 | Valid Acc: 61.31% | Valid F1-Weighted: 59.52%\n",
            "Epoch 8\n",
            "Train Loss: 0.952 | Train Acc: 61.78%\n",
            "Valid Loss: 0.966 | Valid Acc: 61.48% | Valid F1-Weighted: 58.71%\n",
            "Epoch 9\n",
            "Train Loss: 0.940 | Train Acc: 62.46%\n",
            "Valid Loss: 0.959 | Valid Acc: 61.03% | Valid F1-Weighted: 59.70%\n",
            "Epoch 10\n",
            "Train Loss: 0.931 | Train Acc: 62.64%\n",
            "Valid Loss: 0.950 | Valid Acc: 62.01% | Valid F1-Weighted: 59.67%\n"
          ]
        }
      ],
      "source": [
        "# Model hyperparameters\n",
        "# Initialize the model\n",
        "INPUT_DIM = len(vocab)\n",
        "EMBEDDING_DIM = 200\n",
        "HIDDEN_DIM = 300\n",
        "OUTPUT_DIM = 5\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = False\n",
        "DROPOUT = 0.6\n",
        "PAD_IDX = vocab[\"<pad>\"]\n",
        "BATCH_SIZE = 128\n",
        "N_EPOCHS = 10\n",
        "\n",
        "# Model\n",
        "model = SentimentGRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX).to(device)\n",
        "\n",
        "glove_vectors = GloVe(name=\"6B\", dim=EMBEDDING_DIM)\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Use pretrained embeddings\n",
        "embedding_matrix = torch.zeros(INPUT_DIM, EMBEDDING_DIM)\n",
        "for i, token in enumerate(vocab.get_itos()):\n",
        "    if token in glove_vectors.stoi:\n",
        "        embedding_matrix[i] = glove_vectors[token]\n",
        "model.embedding.weight.data.copy_(embedding_matrix)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "\n",
        "nn_res = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
        "    valid_loss, valid_acc,all_preds = evaluate(model, valid_loader, criterion)\n",
        "\n",
        "    y_hat_val = []\n",
        "    for n in range(len(all_preds)):\n",
        "      y_hat_val += [i[0] for i in all_preds[n].cpu().numpy()]\n",
        "\n",
        "    f1_w = f1_score(y_true=valid_dataset.sentiments,\n",
        "         y_pred=y_hat_val,\n",
        "         average='weighted')\n",
        "\n",
        "    print(f'Epoch {epoch+1}')\n",
        "    print(f'Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'Valid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:.2f}% | Valid F1-Weighted: {f1_w*100:.2f}%')\n",
        "\n",
        "    nn_res.append([epoch,train_loss,train_acc,valid_loss,valid_acc,f1_w])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXLn-N2mAUvt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bc307ba-4a16-440d-d8da-314f3a8abce7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Train Loss: 1.154 | Train Acc: 53.91%\n",
            "Valid Loss: 1.059 | Valid Acc: 57.58% | Valid F1-Weighted: 51.43%\n",
            "Epoch 2\n",
            "Train Loss: 1.082 | Train Acc: 56.78%\n",
            "Valid Loss: 1.018 | Valid Acc: 59.49% | Valid F1-Weighted: 55.07%\n",
            "Epoch 3\n",
            "Train Loss: 1.057 | Train Acc: 57.81%\n",
            "Valid Loss: 1.012 | Valid Acc: 59.51% | Valid F1-Weighted: 55.95%\n",
            "Epoch 4\n",
            "Train Loss: 1.042 | Train Acc: 58.36%\n",
            "Valid Loss: 1.000 | Valid Acc: 59.99% | Valid F1-Weighted: 55.39%\n",
            "Epoch 5\n",
            "Train Loss: 1.033 | Train Acc: 58.70%\n",
            "Valid Loss: 1.010 | Valid Acc: 58.88% | Valid F1-Weighted: 53.17%\n",
            "Epoch 6\n",
            "Train Loss: 1.023 | Train Acc: 59.18%\n",
            "Valid Loss: 0.991 | Valid Acc: 60.10% | Valid F1-Weighted: 55.85%\n",
            "Epoch 7\n",
            "Train Loss: 1.017 | Train Acc: 59.36%\n",
            "Valid Loss: 0.992 | Valid Acc: 60.14% | Valid F1-Weighted: 55.32%\n",
            "Epoch 8\n",
            "Train Loss: 1.011 | Train Acc: 59.55%\n",
            "Valid Loss: 0.984 | Valid Acc: 60.42% | Valid F1-Weighted: 56.12%\n",
            "Epoch 9\n",
            "Train Loss: 1.007 | Train Acc: 59.87%\n",
            "Valid Loss: 0.987 | Valid Acc: 60.12% | Valid F1-Weighted: 55.04%\n",
            "Epoch 10\n",
            "Train Loss: 1.002 | Train Acc: 59.94%\n",
            "Valid Loss: 0.973 | Valid Acc: 61.29% | Valid F1-Weighted: 57.87%\n",
            "Epoch 11\n",
            "Train Loss: 0.997 | Train Acc: 60.30%\n",
            "Valid Loss: 0.978 | Valid Acc: 61.03% | Valid F1-Weighted: 56.80%\n",
            "Epoch 12\n",
            "Train Loss: 0.996 | Train Acc: 60.30%\n",
            "Valid Loss: 0.976 | Valid Acc: 61.16% | Valid F1-Weighted: 58.79%\n",
            "Epoch 13\n",
            "Train Loss: 0.994 | Train Acc: 60.42%\n",
            "Valid Loss: 0.984 | Valid Acc: 61.22% | Valid F1-Weighted: 57.82%\n",
            "Epoch 14\n",
            "Train Loss: 0.989 | Train Acc: 60.72%\n",
            "Valid Loss: 0.969 | Valid Acc: 61.32% | Valid F1-Weighted: 58.10%\n",
            "Epoch 15\n",
            "Train Loss: 1.013 | Train Acc: 59.94%\n",
            "Valid Loss: 1.013 | Valid Acc: 59.91% | Valid F1-Weighted: 55.05%\n",
            "Epoch 16\n",
            "Train Loss: 1.020 | Train Acc: 59.81%\n",
            "Valid Loss: 1.020 | Valid Acc: 59.19% | Valid F1-Weighted: 53.23%\n",
            "Epoch 17\n",
            "Train Loss: 1.005 | Train Acc: 60.38%\n",
            "Valid Loss: 0.983 | Valid Acc: 60.86% | Valid F1-Weighted: 57.26%\n",
            "Epoch 18\n",
            "Train Loss: 1.000 | Train Acc: 60.59%\n",
            "Valid Loss: 0.974 | Valid Acc: 61.89% | Valid F1-Weighted: 59.72%\n",
            "Epoch 19\n",
            "Train Loss: 0.991 | Train Acc: 60.92%\n",
            "Valid Loss: 1.000 | Valid Acc: 61.27% | Valid F1-Weighted: 57.47%\n",
            "Epoch 20\n",
            "Train Loss: 0.989 | Train Acc: 60.96%\n",
            "Valid Loss: 0.968 | Valid Acc: 61.55% | Valid F1-Weighted: 58.05%\n"
          ]
        }
      ],
      "source": [
        "# Model hyperparameters\n",
        "# Initialize the model\n",
        "INPUT_DIM = len(vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 5\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.7\n",
        "PAD_IDX = vocab[\"<pad>\"]\n",
        "BATCH_SIZE = 16\n",
        "LR = 0.001\n",
        "WD = 1e-5\n",
        "N_EPOCHS = 20\n",
        "# Model\n",
        "model = SentimentGRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX).to(device)\n",
        "\n",
        "glove_vectors = GloVe(name=\"6B\", dim=EMBEDDING_DIM)\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Use pretrained embeddings\n",
        "embedding_matrix = torch.zeros(INPUT_DIM, EMBEDDING_DIM)\n",
        "for i, token in enumerate(vocab.get_itos()):\n",
        "    if token in glove_vectors.stoi:\n",
        "        embedding_matrix[i] = glove_vectors[token]\n",
        "model.embedding.weight.data.copy_(embedding_matrix)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# Training loop\n",
        "nn_res = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
        "    valid_loss, valid_acc,all_preds = evaluate(model, valid_loader, criterion)\n",
        "\n",
        "    y_hat_val = []\n",
        "    for n in range(len(all_preds)):\n",
        "      y_hat_val += [i[0] for i in all_preds[n].cpu().numpy()]\n",
        "\n",
        "    f1_w = f1_score(y_true=valid_dataset.sentiments,\n",
        "         y_pred=y_hat_val,\n",
        "         average='weighted')\n",
        "\n",
        "    print(f'Epoch {epoch+1}')\n",
        "    print(f'Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'Valid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:.2f}% | Valid F1-Weighted: {f1_w*100:.2f}%')\n",
        "\n",
        "    nn_res.append([epoch,train_loss,train_acc,valid_loss,valid_acc,f1_w])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ptn9r0SBAUvv",
        "outputId": "2c3664bd-0f8b-4cf2-b0b3-7100d8d35940"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Train Loss: 1.241 | Train Acc: 51.02%\n",
            "Valid Loss: 1.215 | Valid Acc: 51.83% | Valid F1-Weighted: 38.25%\n",
            "Epoch 2\n",
            "Train Loss: 1.215 | Train Acc: 51.54%\n",
            "Valid Loss: 1.225 | Valid Acc: 51.70% | Valid F1-Weighted: 37.67%\n",
            "Epoch 3\n",
            "Train Loss: 1.205 | Train Acc: 52.04%\n",
            "Valid Loss: 1.207 | Valid Acc: 52.70% | Valid F1-Weighted: 40.80%\n",
            "Epoch 4\n",
            "Train Loss: 1.193 | Train Acc: 52.51%\n",
            "Valid Loss: 1.164 | Valid Acc: 53.82% | Valid F1-Weighted: 43.99%\n",
            "Epoch 5\n",
            "Train Loss: 1.182 | Train Acc: 52.96%\n",
            "Valid Loss: 1.161 | Valid Acc: 54.02% | Valid F1-Weighted: 44.10%\n",
            "Epoch 6\n",
            "Train Loss: 1.173 | Train Acc: 53.29%\n",
            "Valid Loss: 1.145 | Valid Acc: 54.33% | Valid F1-Weighted: 45.03%\n",
            "Epoch 7\n",
            "Train Loss: 1.166 | Train Acc: 53.63%\n",
            "Valid Loss: 1.136 | Valid Acc: 54.88% | Valid F1-Weighted: 46.83%\n",
            "Epoch 8\n",
            "Train Loss: 1.159 | Train Acc: 53.72%\n",
            "Valid Loss: 1.133 | Valid Acc: 54.68% | Valid F1-Weighted: 45.66%\n",
            "Epoch 9\n",
            "Train Loss: 1.153 | Train Acc: 53.99%\n",
            "Valid Loss: 1.120 | Valid Acc: 55.30% | Valid F1-Weighted: 47.15%\n",
            "Epoch 10\n",
            "Train Loss: 1.147 | Train Acc: 54.30%\n",
            "Valid Loss: 1.117 | Valid Acc: 55.62% | Valid F1-Weighted: 48.26%\n",
            "Epoch 11\n",
            "Train Loss: 1.142 | Train Acc: 54.45%\n",
            "Valid Loss: 1.115 | Valid Acc: 55.48% | Valid F1-Weighted: 47.10%\n",
            "Epoch 12\n",
            "Train Loss: 1.137 | Train Acc: 54.68%\n",
            "Valid Loss: 1.111 | Valid Acc: 55.78% | Valid F1-Weighted: 47.80%\n",
            "Epoch 13\n",
            "Train Loss: 1.134 | Train Acc: 54.74%\n",
            "Valid Loss: 1.098 | Valid Acc: 56.22% | Valid F1-Weighted: 49.75%\n",
            "Epoch 14\n",
            "Train Loss: 1.129 | Train Acc: 54.95%\n",
            "Valid Loss: 1.100 | Valid Acc: 55.89% | Valid F1-Weighted: 48.31%\n",
            "Epoch 15\n",
            "Train Loss: 1.126 | Train Acc: 55.04%\n",
            "Valid Loss: 1.093 | Valid Acc: 56.32% | Valid F1-Weighted: 49.54%\n",
            "Epoch 16\n",
            "Train Loss: 1.121 | Train Acc: 55.32%\n",
            "Valid Loss: 1.087 | Valid Acc: 56.64% | Valid F1-Weighted: 50.19%\n",
            "Epoch 17\n",
            "Train Loss: 1.119 | Train Acc: 55.51%\n",
            "Valid Loss: 1.086 | Valid Acc: 56.70% | Valid F1-Weighted: 50.48%\n",
            "Epoch 18\n",
            "Train Loss: 1.117 | Train Acc: 55.50%\n",
            "Valid Loss: 1.094 | Valid Acc: 56.31% | Valid F1-Weighted: 49.25%\n",
            "Epoch 19\n",
            "Train Loss: 1.111 | Train Acc: 55.81%\n",
            "Valid Loss: 1.070 | Valid Acc: 57.06% | Valid F1-Weighted: 51.03%\n",
            "Epoch 20\n",
            "Train Loss: 1.109 | Train Acc: 55.79%\n",
            "Valid Loss: 1.069 | Valid Acc: 57.06% | Valid F1-Weighted: 51.10%\n"
          ]
        }
      ],
      "source": [
        "# Model hyperparameters\n",
        "# Initialize the model\n",
        "INPUT_DIM = len(vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 5\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.7\n",
        "PAD_IDX = vocab[\"<pad>\"]\n",
        "BATCH_SIZE = 32\n",
        "N_EPOCHS = 20\n",
        "\n",
        "# Model\n",
        "model = SentimentGRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX).to(device)\n",
        "\n",
        "# glove_vectors = GloVe(name=\"6B\", dim=EMBEDDING_DIM)\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Use pretrained embeddings\n",
        "# embedding_matrix = torch.zeros(INPUT_DIM, EMBEDDING_DIM)\n",
        "# for i, token in enumerate(vocab.get_itos()):\n",
        "#     if token in glove_vectors.stoi:\n",
        "#         embedding_matrix[i] = glove_vectors[token]\n",
        "# model.embedding.weight.data.copy_(embedding_matrix)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(lr = 0.0001,params=model.parameters())\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "nn_res = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
        "    valid_loss, valid_acc,all_preds = evaluate(model, valid_loader, criterion)\n",
        "\n",
        "    y_hat_val = []\n",
        "    for n in range(len(all_preds)):\n",
        "      y_hat_val += [i[0] for i in all_preds[n].cpu().numpy()]\n",
        "\n",
        "    f1_w = f1_score(y_true=valid_dataset.sentiments,\n",
        "         y_pred=y_hat_val,\n",
        "         average='weighted')\n",
        "\n",
        "    print(f'Epoch {epoch+1}')\n",
        "    print(f'Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'Valid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:.2f}% | Valid F1-Weighted: {f1_w*100:.2f}%')\n",
        "\n",
        "    nn_res.append([epoch,train_loss,train_acc,valid_loss,valid_acc,f1_w])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYlq7BYoAUvy",
        "outputId": "2e901f46-52b9-4b87-c2f9-2d4cd2e98421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Train Loss: 1.153 | Train Acc: 53.73%\n",
            "Valid Loss: 1.064 | Valid Acc: 56.97% | Valid F1-Weighted: 50.48%\n",
            "Epoch 2\n",
            "Train Loss: 1.091 | Train Acc: 56.01%\n",
            "Valid Loss: 1.034 | Valid Acc: 58.11% | Valid F1-Weighted: 52.35%\n",
            "Epoch 3\n",
            "Train Loss: 1.061 | Train Acc: 57.27%\n",
            "Valid Loss: 1.008 | Valid Acc: 59.63% | Valid F1-Weighted: 56.88%\n",
            "Epoch 4\n",
            "Train Loss: 1.079 | Train Acc: 56.98%\n",
            "Valid Loss: 1.069 | Valid Acc: 57.83% | Valid F1-Weighted: 51.13%\n",
            "Epoch 5\n",
            "Train Loss: 1.056 | Train Acc: 57.70%\n",
            "Valid Loss: 0.993 | Valid Acc: 60.02% | Valid F1-Weighted: 57.30%\n",
            "Epoch 6\n",
            "Train Loss: 1.032 | Train Acc: 58.36%\n",
            "Valid Loss: 0.993 | Valid Acc: 60.33% | Valid F1-Weighted: 58.48%\n",
            "Epoch 7\n",
            "Train Loss: 1.022 | Train Acc: 58.75%\n",
            "Valid Loss: 0.994 | Valid Acc: 60.36% | Valid F1-Weighted: 58.92%\n",
            "Epoch 8\n",
            "Train Loss: 1.014 | Train Acc: 59.48%\n",
            "Valid Loss: 0.988 | Valid Acc: 60.48% | Valid F1-Weighted: 57.09%\n",
            "Epoch 9\n",
            "Train Loss: 1.003 | Train Acc: 59.56%\n",
            "Valid Loss: 0.972 | Valid Acc: 61.07% | Valid F1-Weighted: 58.82%\n",
            "Epoch 10\n",
            "Train Loss: 0.992 | Train Acc: 60.26%\n",
            "Valid Loss: 0.972 | Valid Acc: 61.11% | Valid F1-Weighted: 59.42%\n",
            "Epoch 11\n",
            "Train Loss: 0.986 | Train Acc: 60.42%\n",
            "Valid Loss: 0.966 | Valid Acc: 61.55% | Valid F1-Weighted: 59.46%\n",
            "Epoch 12\n",
            "Train Loss: 0.978 | Train Acc: 60.71%\n",
            "Valid Loss: 0.966 | Valid Acc: 61.56% | Valid F1-Weighted: 59.04%\n",
            "Epoch 13\n",
            "Train Loss: 0.970 | Train Acc: 61.14%\n",
            "Valid Loss: 0.962 | Valid Acc: 61.45% | Valid F1-Weighted: 59.32%\n",
            "Epoch 14\n",
            "Train Loss: 0.963 | Train Acc: 61.47%\n",
            "Valid Loss: 0.961 | Valid Acc: 61.51% | Valid F1-Weighted: 59.73%\n",
            "Epoch 15\n",
            "Train Loss: 0.957 | Train Acc: 61.72%\n",
            "Valid Loss: 0.957 | Valid Acc: 61.76% | Valid F1-Weighted: 59.78%\n",
            "Epoch 16\n",
            "Train Loss: 0.952 | Train Acc: 61.93%\n",
            "Valid Loss: 0.953 | Valid Acc: 61.99% | Valid F1-Weighted: 59.85%\n",
            "Epoch 17\n",
            "Train Loss: 0.948 | Train Acc: 62.16%\n",
            "Valid Loss: 0.952 | Valid Acc: 61.65% | Valid F1-Weighted: 59.48%\n",
            "Epoch 18\n",
            "Train Loss: 0.948 | Train Acc: 62.28%\n",
            "Valid Loss: 0.951 | Valid Acc: 62.15% | Valid F1-Weighted: 60.46%\n",
            "Epoch 19\n",
            "Train Loss: 0.940 | Train Acc: 62.49%\n",
            "Valid Loss: 0.952 | Valid Acc: 61.86% | Valid F1-Weighted: 59.09%\n",
            "Epoch 20\n",
            "Train Loss: 0.936 | Train Acc: 62.66%\n",
            "Valid Loss: 0.952 | Valid Acc: 61.90% | Valid F1-Weighted: 60.33%\n",
            "Epoch 21\n",
            "Train Loss: 0.930 | Train Acc: 62.81%\n",
            "Valid Loss: 0.957 | Valid Acc: 61.35% | Valid F1-Weighted: 60.40%\n",
            "Epoch 22\n",
            "Train Loss: 0.930 | Train Acc: 62.87%\n",
            "Valid Loss: 0.960 | Valid Acc: 61.59% | Valid F1-Weighted: 60.63%\n",
            "Epoch 23\n",
            "Train Loss: 0.926 | Train Acc: 63.00%\n",
            "Valid Loss: 0.955 | Valid Acc: 61.69% | Valid F1-Weighted: 58.88%\n",
            "Epoch 24\n",
            "Train Loss: 0.923 | Train Acc: 63.09%\n",
            "Valid Loss: 0.951 | Valid Acc: 61.88% | Valid F1-Weighted: 60.19%\n",
            "Epoch 25\n",
            "Train Loss: 0.922 | Train Acc: 63.13%\n",
            "Valid Loss: 0.941 | Valid Acc: 62.30% | Valid F1-Weighted: 60.23%\n",
            "Epoch 26\n",
            "Train Loss: 0.918 | Train Acc: 63.37%\n",
            "Valid Loss: 0.942 | Valid Acc: 62.42% | Valid F1-Weighted: 60.30%\n",
            "Epoch 27\n",
            "Train Loss: 0.916 | Train Acc: 63.38%\n",
            "Valid Loss: 0.948 | Valid Acc: 61.98% | Valid F1-Weighted: 60.34%\n",
            "Epoch 28\n",
            "Train Loss: 0.914 | Train Acc: 63.47%\n",
            "Valid Loss: 0.954 | Valid Acc: 62.41% | Valid F1-Weighted: 60.63%\n",
            "Epoch 29\n",
            "Train Loss: 0.912 | Train Acc: 63.58%\n",
            "Valid Loss: 0.958 | Valid Acc: 61.56% | Valid F1-Weighted: 58.40%\n",
            "Epoch 30\n",
            "Train Loss: 0.910 | Train Acc: 63.61%\n",
            "Valid Loss: 0.946 | Valid Acc: 62.44% | Valid F1-Weighted: 60.57%\n",
            "Epoch 31\n",
            "Train Loss: 0.910 | Train Acc: 63.59%\n",
            "Valid Loss: 0.953 | Valid Acc: 61.85% | Valid F1-Weighted: 59.67%\n",
            "Epoch 32\n",
            "Train Loss: 0.906 | Train Acc: 63.87%\n",
            "Valid Loss: 0.950 | Valid Acc: 62.21% | Valid F1-Weighted: 60.63%\n",
            "Epoch 33\n",
            "Train Loss: 0.906 | Train Acc: 63.84%\n",
            "Valid Loss: 0.948 | Valid Acc: 62.34% | Valid F1-Weighted: 60.96%\n",
            "Epoch 34\n",
            "Train Loss: 0.904 | Train Acc: 64.02%\n",
            "Valid Loss: 0.951 | Valid Acc: 62.23% | Valid F1-Weighted: 60.48%\n",
            "Epoch 35\n",
            "Train Loss: 0.903 | Train Acc: 64.18%\n",
            "Valid Loss: 0.941 | Valid Acc: 62.74% | Valid F1-Weighted: 60.88%\n",
            "Epoch 36\n",
            "Train Loss: 0.902 | Train Acc: 64.00%\n",
            "Valid Loss: 0.944 | Valid Acc: 62.41% | Valid F1-Weighted: 59.72%\n",
            "Epoch 37\n",
            "Train Loss: 0.901 | Train Acc: 64.00%\n",
            "Valid Loss: 0.943 | Valid Acc: 62.28% | Valid F1-Weighted: 59.94%\n",
            "Epoch 38\n",
            "Train Loss: 0.900 | Train Acc: 64.10%\n",
            "Valid Loss: 0.946 | Valid Acc: 62.30% | Valid F1-Weighted: 60.18%\n",
            "Epoch 39\n",
            "Train Loss: 0.899 | Train Acc: 64.20%\n",
            "Valid Loss: 0.944 | Valid Acc: 62.37% | Valid F1-Weighted: 60.04%\n",
            "Epoch 40\n",
            "Train Loss: 0.899 | Train Acc: 64.29%\n",
            "Valid Loss: 0.945 | Valid Acc: 62.26% | Valid F1-Weighted: 59.64%\n",
            "Epoch 41\n",
            "Train Loss: 0.898 | Train Acc: 64.05%\n",
            "Valid Loss: 0.947 | Valid Acc: 62.36% | Valid F1-Weighted: 60.14%\n",
            "Epoch 42\n",
            "Train Loss: 0.897 | Train Acc: 64.28%\n",
            "Valid Loss: 0.944 | Valid Acc: 62.48% | Valid F1-Weighted: 60.66%\n",
            "Epoch 43\n",
            "Train Loss: 0.900 | Train Acc: 64.03%\n",
            "Valid Loss: 0.944 | Valid Acc: 62.52% | Valid F1-Weighted: 60.48%\n",
            "Epoch 44\n",
            "Train Loss: 0.896 | Train Acc: 64.28%\n",
            "Valid Loss: 0.944 | Valid Acc: 62.05% | Valid F1-Weighted: 59.33%\n",
            "Epoch 45\n",
            "Train Loss: 0.897 | Train Acc: 64.37%\n",
            "Valid Loss: 0.943 | Valid Acc: 62.41% | Valid F1-Weighted: 59.57%\n",
            "Epoch 46\n",
            "Train Loss: 0.895 | Train Acc: 64.35%\n",
            "Valid Loss: 0.947 | Valid Acc: 62.46% | Valid F1-Weighted: 60.37%\n",
            "Epoch 47\n",
            "Train Loss: 0.896 | Train Acc: 64.31%\n",
            "Valid Loss: 0.944 | Valid Acc: 62.30% | Valid F1-Weighted: 59.46%\n",
            "Epoch 48\n",
            "Train Loss: 0.896 | Train Acc: 64.21%\n",
            "Valid Loss: 0.955 | Valid Acc: 62.28% | Valid F1-Weighted: 60.29%\n",
            "Epoch 49\n",
            "Train Loss: 0.897 | Train Acc: 64.12%\n",
            "Valid Loss: 0.947 | Valid Acc: 62.29% | Valid F1-Weighted: 59.67%\n",
            "Epoch 50\n",
            "Train Loss: 0.896 | Train Acc: 64.11%\n",
            "Valid Loss: 0.945 | Valid Acc: 62.33% | Valid F1-Weighted: 59.52%\n",
            "Epoch 51\n",
            "Train Loss: 0.894 | Train Acc: 64.19%\n",
            "Valid Loss: 0.946 | Valid Acc: 62.52% | Valid F1-Weighted: 60.12%\n",
            "Epoch 52\n",
            "Train Loss: 0.898 | Train Acc: 64.26%\n",
            "Valid Loss: 0.947 | Valid Acc: 62.62% | Valid F1-Weighted: 60.47%\n",
            "Epoch 53\n",
            "Train Loss: 0.895 | Train Acc: 64.30%\n",
            "Valid Loss: 0.951 | Valid Acc: 62.18% | Valid F1-Weighted: 59.96%\n",
            "Epoch 54\n",
            "Train Loss: 0.897 | Train Acc: 64.30%\n",
            "Valid Loss: 0.953 | Valid Acc: 62.19% | Valid F1-Weighted: 60.44%\n",
            "Epoch 55\n",
            "Train Loss: 0.898 | Train Acc: 64.01%\n",
            "Valid Loss: 0.948 | Valid Acc: 62.47% | Valid F1-Weighted: 59.85%\n",
            "Epoch 56\n",
            "Train Loss: 0.897 | Train Acc: 64.13%\n",
            "Valid Loss: 0.946 | Valid Acc: 62.45% | Valid F1-Weighted: 59.61%\n",
            "Epoch 57\n",
            "Train Loss: 0.894 | Train Acc: 64.27%\n",
            "Valid Loss: 0.949 | Valid Acc: 62.19% | Valid F1-Weighted: 59.21%\n",
            "Epoch 58\n",
            "Train Loss: 0.898 | Train Acc: 64.15%\n",
            "Valid Loss: 0.951 | Valid Acc: 61.76% | Valid F1-Weighted: 58.29%\n",
            "Epoch 59\n",
            "Train Loss: 0.898 | Train Acc: 64.17%\n",
            "Valid Loss: 0.952 | Valid Acc: 62.26% | Valid F1-Weighted: 60.42%\n",
            "Epoch 60\n",
            "Train Loss: 0.895 | Train Acc: 64.12%\n",
            "Valid Loss: 0.947 | Valid Acc: 62.35% | Valid F1-Weighted: 59.73%\n",
            "Epoch 61\n",
            "Train Loss: 0.897 | Train Acc: 64.21%\n",
            "Valid Loss: 0.955 | Valid Acc: 62.31% | Valid F1-Weighted: 60.01%\n",
            "Epoch 62\n",
            "Train Loss: 0.898 | Train Acc: 64.14%\n",
            "Valid Loss: 0.968 | Valid Acc: 62.05% | Valid F1-Weighted: 59.98%\n",
            "Epoch 63\n",
            "Train Loss: 0.900 | Train Acc: 64.01%\n",
            "Valid Loss: 0.950 | Valid Acc: 62.42% | Valid F1-Weighted: 59.89%\n",
            "Epoch 64\n",
            "Train Loss: 0.901 | Train Acc: 64.05%\n",
            "Valid Loss: 0.953 | Valid Acc: 61.82% | Valid F1-Weighted: 58.76%\n",
            "Epoch 65\n",
            "Train Loss: 0.900 | Train Acc: 63.99%\n",
            "Valid Loss: 0.954 | Valid Acc: 62.32% | Valid F1-Weighted: 59.57%\n",
            "Epoch 66\n",
            "Train Loss: 0.900 | Train Acc: 64.02%\n",
            "Valid Loss: 0.952 | Valid Acc: 62.03% | Valid F1-Weighted: 59.11%\n",
            "Epoch 67\n",
            "Train Loss: 0.903 | Train Acc: 63.93%\n",
            "Valid Loss: 0.956 | Valid Acc: 62.24% | Valid F1-Weighted: 59.74%\n",
            "Epoch 68\n",
            "Train Loss: 0.902 | Train Acc: 63.93%\n",
            "Valid Loss: 0.958 | Valid Acc: 61.97% | Valid F1-Weighted: 58.89%\n",
            "Epoch 69\n",
            "Train Loss: 0.904 | Train Acc: 63.81%\n",
            "Valid Loss: 0.954 | Valid Acc: 62.21% | Valid F1-Weighted: 59.56%\n",
            "Epoch 70\n",
            "Train Loss: 0.903 | Train Acc: 63.87%\n",
            "Valid Loss: 0.959 | Valid Acc: 62.15% | Valid F1-Weighted: 60.04%\n",
            "Epoch 71\n",
            "Train Loss: 0.903 | Train Acc: 63.88%\n",
            "Valid Loss: 0.963 | Valid Acc: 62.31% | Valid F1-Weighted: 59.85%\n",
            "Epoch 72\n",
            "Train Loss: 0.906 | Train Acc: 63.74%\n",
            "Valid Loss: 0.953 | Valid Acc: 62.51% | Valid F1-Weighted: 59.83%\n",
            "Epoch 73\n",
            "Train Loss: 0.907 | Train Acc: 63.69%\n",
            "Valid Loss: 0.959 | Valid Acc: 61.73% | Valid F1-Weighted: 58.03%\n",
            "Epoch 74\n",
            "Train Loss: 0.907 | Train Acc: 63.73%\n",
            "Valid Loss: 0.961 | Valid Acc: 62.13% | Valid F1-Weighted: 59.01%\n",
            "Epoch 75\n",
            "Train Loss: 0.909 | Train Acc: 63.66%\n",
            "Valid Loss: 0.958 | Valid Acc: 62.31% | Valid F1-Weighted: 60.15%\n",
            "Epoch 76\n",
            "Train Loss: 0.912 | Train Acc: 63.39%\n",
            "Valid Loss: 0.962 | Valid Acc: 61.59% | Valid F1-Weighted: 57.96%\n",
            "Epoch 77\n",
            "Train Loss: 0.915 | Train Acc: 63.34%\n",
            "Valid Loss: 0.977 | Valid Acc: 61.02% | Valid F1-Weighted: 57.54%\n",
            "Epoch 78\n",
            "Train Loss: 0.918 | Train Acc: 63.28%\n",
            "Valid Loss: 0.963 | Valid Acc: 61.91% | Valid F1-Weighted: 59.05%\n",
            "Epoch 79\n",
            "Train Loss: 0.925 | Train Acc: 63.03%\n",
            "Valid Loss: 0.970 | Valid Acc: 61.34% | Valid F1-Weighted: 57.93%\n",
            "Epoch 80\n",
            "Train Loss: 0.927 | Train Acc: 62.79%\n",
            "Valid Loss: 0.967 | Valid Acc: 61.56% | Valid F1-Weighted: 58.18%\n",
            "Epoch 81\n",
            "Train Loss: 0.928 | Train Acc: 62.94%\n",
            "Valid Loss: 0.972 | Valid Acc: 60.89% | Valid F1-Weighted: 56.80%\n",
            "Epoch 82\n",
            "Train Loss: 0.926 | Train Acc: 62.92%\n",
            "Valid Loss: 0.967 | Valid Acc: 61.42% | Valid F1-Weighted: 58.15%\n",
            "Epoch 83\n",
            "Train Loss: 0.924 | Train Acc: 62.98%\n",
            "Valid Loss: 0.964 | Valid Acc: 61.71% | Valid F1-Weighted: 58.79%\n",
            "Epoch 84\n",
            "Train Loss: 0.924 | Train Acc: 62.96%\n",
            "Valid Loss: 0.974 | Valid Acc: 60.91% | Valid F1-Weighted: 56.85%\n",
            "Epoch 85\n",
            "Train Loss: 0.921 | Train Acc: 63.04%\n",
            "Valid Loss: 0.967 | Valid Acc: 61.44% | Valid F1-Weighted: 57.76%\n",
            "Epoch 86\n",
            "Train Loss: 0.930 | Train Acc: 62.79%\n",
            "Valid Loss: 0.979 | Valid Acc: 61.41% | Valid F1-Weighted: 58.25%\n",
            "Epoch 87\n",
            "Train Loss: 0.926 | Train Acc: 62.87%\n",
            "Valid Loss: 0.971 | Valid Acc: 61.21% | Valid F1-Weighted: 57.36%\n",
            "Epoch 88\n",
            "Train Loss: 0.928 | Train Acc: 62.68%\n",
            "Valid Loss: 0.973 | Valid Acc: 61.40% | Valid F1-Weighted: 57.99%\n"
          ]
        }
      ],
      "source": [
        "# Model hyperparameters\n",
        "# Initialize the model\n",
        "INPUT_DIM = len(vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 512\n",
        "OUTPUT_DIM = 5\n",
        "N_LAYERS = 3\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.6\n",
        "PAD_IDX = vocab[\"<pad>\"]\n",
        "BATCH_SIZE = 128\n",
        "N_EPOCHS = 100\n",
        "\n",
        "# Model\n",
        "model = SentimentGRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX).to(device)\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "glove_vectors = GloVe(name=\"6B\", dim=EMBEDDING_DIM)\n",
        "\n",
        "# Use pretrained embeddings\n",
        "embedding_matrix = torch.zeros(INPUT_DIM, EMBEDDING_DIM)\n",
        "for i, token in enumerate(vocab.get_itos()):\n",
        "    if token in glove_vectors.stoi:\n",
        "        embedding_matrix[i] = glove_vectors[token]\n",
        "model.embedding.weight.data.copy_(embedding_matrix)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# Training loop\n",
        "nn_res = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
        "    valid_loss, valid_acc,all_preds = evaluate(model, valid_loader, criterion)\n",
        "\n",
        "    y_hat_val = []\n",
        "    for n in range(len(all_preds)):\n",
        "      y_hat_val += [i[0] for i in all_preds[n].cpu().numpy()]\n",
        "\n",
        "    f1_w = f1_score(y_true=valid_dataset.sentiments,\n",
        "         y_pred=y_hat_val,\n",
        "         average='weighted')\n",
        "\n",
        "    print(f'Epoch {epoch+1}')\n",
        "    print(f'Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'Valid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:.2f}% | Valid F1-Weighted: {f1_w*100:.2f}%')\n",
        "\n",
        "    nn_res.append([epoch,train_loss,train_acc,valid_loss,valid_acc,f1_w])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "rk5abwJ1rEYo",
        "outputId": "6b0e0e6e-2220-4cfb-ee36-e96501554ea6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"results\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"total_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 135911.5878030378,\n        \"min\": 367998.0,\n        \"max\": 677364.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          677364.0,\n          367998.0,\n          676776.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vocab_sizes\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2823.5795248820837,\n        \"min\": 11622.0,\n        \"max\": 18087.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          18087.0,\n          16465.0,\n          11622.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"training_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.043724211095889,\n        \"min\": 10.973437070846558,\n        \"max\": 18.545607089996338,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          16.007943153381348,\n          17.848891735076904,\n          10.973437070846558\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0011520766820447559,\n        \"min\": 0.5915628508795381,\n        \"max\": 0.5948243597283858,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.5942362187884297,\n          0.5915628508795381,\n          0.5930064695503395\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0037185645977028145,\n        \"min\": 0.5757746201608549,\n        \"max\": 0.5853228194020819,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.5853228194020819,\n          0.5843471274685119,\n          0.5762634543408777\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "results"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-f5f9f2d4-9dc0-4f6b-8b2b-38f173537f9e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>total_tokens</th>\n",
              "      <th>vocab_sizes</th>\n",
              "      <th>training_time</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>f1_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>caso_0</th>\n",
              "      <td>676776.0</td>\n",
              "      <td>18087.0</td>\n",
              "      <td>16.007943</td>\n",
              "      <td>0.594236</td>\n",
              "      <td>0.585323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>caso_1</th>\n",
              "      <td>677364.0</td>\n",
              "      <td>16465.0</td>\n",
              "      <td>17.848892</td>\n",
              "      <td>0.594236</td>\n",
              "      <td>0.584347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>caso_2</th>\n",
              "      <td>428043.0</td>\n",
              "      <td>16334.0</td>\n",
              "      <td>18.545607</td>\n",
              "      <td>0.591563</td>\n",
              "      <td>0.579889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>caso_3</th>\n",
              "      <td>428043.0</td>\n",
              "      <td>11973.0</td>\n",
              "      <td>13.133028</td>\n",
              "      <td>0.594824</td>\n",
              "      <td>0.578652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>caso_4</th>\n",
              "      <td>428043.0</td>\n",
              "      <td>11953.0</td>\n",
              "      <td>11.305312</td>\n",
              "      <td>0.594717</td>\n",
              "      <td>0.578476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>caso_5</th>\n",
              "      <td>367998.0</td>\n",
              "      <td>11622.0</td>\n",
              "      <td>10.973437</td>\n",
              "      <td>0.593488</td>\n",
              "      <td>0.576263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>caso_6</th>\n",
              "      <td>367998.0</td>\n",
              "      <td>11622.0</td>\n",
              "      <td>13.390586</td>\n",
              "      <td>0.593006</td>\n",
              "      <td>0.575775</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5f9f2d4-9dc0-4f6b-8b2b-38f173537f9e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f5f9f2d4-9dc0-4f6b-8b2b-38f173537f9e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f5f9f2d4-9dc0-4f6b-8b2b-38f173537f9e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3f7bfe49-3e7b-4d4d-b640-8fb0cbb9dabc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3f7bfe49-3e7b-4d4d-b640-8fb0cbb9dabc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3f7bfe49-3e7b-4d4d-b640-8fb0cbb9dabc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_086904d2-5779-4056-9508-f194fd07a0d5\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_086904d2-5779-4056-9508-f194fd07a0d5 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "        total_tokens  vocab_sizes  training_time  accuracy  f1_score\n",
              "caso_0      676776.0      18087.0      16.007943  0.594236  0.585323\n",
              "caso_1      677364.0      16465.0      17.848892  0.594236  0.584347\n",
              "caso_2      428043.0      16334.0      18.545607  0.591563  0.579889\n",
              "caso_3      428043.0      11973.0      13.133028  0.594824  0.578652\n",
              "caso_4      428043.0      11953.0      11.305312  0.594717  0.578476\n",
              "caso_5      367998.0      11622.0      10.973437  0.593488  0.576263\n",
              "caso_6      367998.0      11622.0      13.390586  0.593006  0.575775"
            ]
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K05i5ks2IIQ"
      },
      "source": [
        "Se aplican acumulativamente las técnicas de preprocesamiento para texto:\n",
        "\n",
        "- Case folding\n",
        "- remove stop words\n",
        "- stemming\n",
        "- lemmatization\n",
        "- remove special characters and numbers\n",
        "- instead of using a 0/1 (binary) vector, use the number of times that a word occurs in the text\n",
        "\n",
        "Cada preprocesamiento reduce la cantidad de variables predictoras de nuestro modelo, pasando de un BoW completo de 18087, a 16465 gracias Case Folding (caso 1), 11622 para el caso 5 (case folding, stop word removing, stemming, lemmatization, numbers and special characters remove). El caso 6 (caso 5 pero en lugar de variables 1/0 se incluye el conteo de ocurrencia) presenta la misma cantidad de variables predictoras que el caso 5. Gracias a esta constante reducción en la dimensionalidad del problema de clasificación se observa una constante reducción en el tiempo de entrenamiento.\n",
        "\n",
        "Se utilizan las metodologías de preprocesamiento de texto por defecto de la famosa librería `NLTK`, un análisis más exhaustivo de diferentes versiones de este preprocesamiento y calibración de sus hyperparámetros es recomendado para futuras investigaciones.\n",
        "\n",
        "Con respecto a las metricas del accuracy y f1-score se observa una ligera reducción a medida que se implementa cada regla de procesamiento de texto. Estas diferencias podrían no ser estadísticamente significativas al momento de aplicar k-fold cross-validation. En el caso de ser diferencias significativas, se debería analizar la posibilidad de cambio de versión o hyperparámetros de la respectiva técnica.\n",
        "\n",
        "Una metrica alta del modelo en el analsis de sentimientos no era objetivo de este estudio, sin embargo, mediante un modelo simple se puede obtener un accuracy de 0.59, un valor considerablemente alto al compararse con el campeón de Kaggle (0.76). Se considera valioso la visualización del preprocesamiento de texto sobre la eficiencia y métricas del modelo."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c88e42-50d4-4b4d-bdf1-894bcff1a41d",
   "metadata": {},
   "source": [
    "# NLP and Neural Networks\n",
    "\n",
    "In this exercise, we'll apply our knowledge of neural networks to process natural language. As we did in the bigram exercise, the goal of this lab is to predict the next word, given the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5132d376-54d7-48c3-a52c-ac3d94ed798b",
   "metadata": {},
   "source": [
    "### Data set\n",
    "\n",
    "Load the text from \"One Hundred Years of Solitude\" that we used in our bigrams exercise. It's located in the data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e309d79-7746-40e3-8a02-3cc7b45c16ac",
   "metadata": {},
   "source": [
    "### Important note:\n",
    "\n",
    "Start with a smaller part of the text. Maybe the first 10 parragraphs, as the number of tokens rapidly increases as we add more text. \n",
    "\n",
    "Later you can use a bigger corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9bbced32-a252-48b0-bc8f-cecfdcf1ec2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_available()=True\n"
     ]
    }
   ],
   "source": [
    "# General libraries\n",
    "import numpy as np\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "print(f\"{torch.cuda.is_available()=}\")\n",
    "\n",
    "# General NLP\n",
    "from nltk import bigrams\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "# General ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f6d3acc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokens)=6293\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "text = open('cap1.txt', 'r').read().lower()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f\"{len(tokens)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1393d6-7cbf-4e8e-a699-0f0cd28982a3",
   "metadata": {},
   "source": [
    "Don't forget to prepare the data by generating the corresponding tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7681843a-18f0-4d7c-9b02-83015f4383e1",
   "metadata": {},
   "source": [
    "### Let's prepare the data set.\n",
    "\n",
    "Our neural network needs to have an input X and an output y. Remember that these sets are numerical, so you'd need something to map the tokens into numbers, and viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c820ccde-c2ee-41ef-b840-41ccca58b550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this case, let's consider a bigram (w1, w2)\n",
    "# assign the w1 to the X vector, and w2 to the y vector, why do we do this?\n",
    "\n",
    "## Our objective is to predict the next word of any given initial word. For this purpouse we use bigrams\n",
    "## then is natural to use a simple classification model where y is the second element of the bigram\n",
    "## and x is the first one. We can extend this idea for any classification model. But, be careful with overfitting! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9cee80bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_list = list(bigrams(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "29c10640-f146-478a-a1a4-d2e747af5ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and labels\n",
    "X = [bigram[0] for bigram in bigram_list]  # First word of the bigram as features\n",
    "y = [bigram[1] for bigram in bigram_list]  # Second word of the bigram as labels\n",
    "\n",
    "# Convert words to a bag-of-words model\n",
    "vectorizer = CountVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "33929328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 10.88%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=0)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cfedfd3b-0396-456b-b9f9-f1c07262a721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget that since we are using torch, our training set vectors should be tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "640c4732-8463-494a-904f-3880975e2273",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_vectorized.toarray(), dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_encoded, dtype=torch.long)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5984fd00-bdbf-4403-a341-b7ef83138db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that our vectors are integers, which can be thought as a categorical variables.\n",
    "# torch provides the one_hot method, that would generate tensors suitable for our nn\n",
    "# make sure that the dtype of your tensor is float."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda25114-c6ae-4e07-a743-12e10cd77796",
   "metadata": {},
   "source": [
    "### Network design\n",
    "To start, we are going to have a very simple network. Define a single layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "82bfac7a-e670-4aaf-bf30-5aa8d0ca46e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many neurons should our input layer have?\n",
    "# Use as many neurons as the total number of categories (from your one-hot encoded tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b6fa6be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6292, 1999])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "99c50d1e-3842-451e-8ede-1c68dddf3843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "    \n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 6292 \n",
    "output_size = len(label_encoder.classes_)\n",
    "\n",
    "model = SimpleNN(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b62f0fdc-bfb8-4081-acfa-bcab5226b82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Train Loss: 7.6144, Train Accuracy: 12.54, Test Accuracy: 7.39\n",
      "Epoch [20/100], Train Loss: 7.5725, Train Accuracy: 13.73, Test Accuracy: 8.34\n",
      "Epoch [30/100], Train Loss: 7.5297, Train Accuracy: 13.73, Test Accuracy: 8.34\n",
      "Epoch [40/100], Train Loss: 7.5040, Train Accuracy: 16.91, Test Accuracy: 7.63\n",
      "Epoch [50/100], Train Loss: 7.4957, Train Accuracy: 17.27, Test Accuracy: 8.02\n",
      "Epoch [60/100], Train Loss: 7.4922, Train Accuracy: 17.44, Test Accuracy: 8.10\n",
      "Epoch [70/100], Train Loss: 7.4849, Train Accuracy: 18.46, Test Accuracy: 8.10\n",
      "Epoch [80/100], Train Loss: 7.4770, Train Accuracy: 18.82, Test Accuracy: 7.94\n",
      "Epoch [90/100], Train Loss: 7.4607, Train Accuracy: 21.04, Test Accuracy: 8.82\n",
      "Epoch [100/100], Train Loss: 7.4516, Train Accuracy: 21.50, Test Accuracy: 9.45\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 100 \n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = accuracy_score(y_train, predicted)\n",
    "\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        accuracy_test = accuracy_score(y_test, predicted)\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Train Accuracy: {accuracy * 100:.2f}, Test Accuracy: {accuracy_test * 100:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "080a6352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the NN is set to one hidden layer with 6292 neurons (same number as the total of y categories)\n",
    "# the model clearly overfits, since we can see a train accuracy of 21.5% against 9.45% in test.\n",
    "# Accuracy for our problem is controversial since we have heavily unbalanced data. But we can see that a simple logistic\n",
    "# regression outperforms our NN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2d09aa-8a47-4668-b1b1-5080be8851ed",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d06d9c5-4df1-4145-812f-ff86958154c1",
   "metadata": {},
   "source": [
    "1. Test your network with a few words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1ac920e4-e9fc-4760-b031-9c959d78bae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aureliano , y y y y y y y y y\n",
      "aureliano , y y y y y y y y y\n",
      "aureliano , y y y y y y y y y\n",
      "aureliano , y y y y y y y y y\n",
      "aureliano , y y y y y y y y y\n",
      "aureliano , y y y y y y y y y\n",
      "aureliano , y y y y y y y y y\n",
      "aureliano , y y y y y y y y y\n",
      "aureliano , y y y y y y y y y\n",
      "aureliano , y y y y y y y y y\n"
     ]
    }
   ],
   "source": [
    "def predict_next_word(input_word):\n",
    "    # Vectorize the input word\n",
    "    input_vectorized = vectorizer.transform([input_word]).toarray()\n",
    "    input_tensor = torch.tensor(input_vectorized, dtype=torch.float32)\n",
    "\n",
    "    # Predict the next word\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "        predicted_prob, predicted_idx = torch.max(probabilities.data, 1)\n",
    "    \n",
    "    # Decode the predicted word\n",
    "    predicted_word = label_encoder.inverse_transform(predicted_idx.numpy())[0]\n",
    "    predicted_prob = predicted_prob.item()  # Convert tensor to Python float\n",
    "\n",
    "    return predicted_word, predicted_prob\n",
    "\n",
    "\n",
    "predict_next_word('Aureliano')\n",
    "\n",
    "\n",
    "\n",
    "max_n_pred = 10\n",
    "for _ in range(10):\n",
    "    word = 'aureliano'\n",
    "    full_pred = word\n",
    "    for i in range(max_n_pred):\n",
    "        word2 = predict_next_word(word)[0]\n",
    "        full_pred = full_pred + ' ' + word2\n",
    "        word = word2\n",
    "\n",
    "    print(full_pred)\n",
    "\n",
    "# Since we freeze the coefficients of our model, we can expect a deterministic output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e724d3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buendia y  - neg. log-ll: -6.903250226233657\n",
      "niño de  - neg. log-ll: -6.679355235165253\n",
      "épocas y  - neg. log-ll: -6.893996545900883\n",
      "pasadas y  - neg. log-ll: -6.907854538680285\n",
      "amaestrado y  - neg. log-ll: -6.904289854528606\n",
      "artificios y  - neg. log-ll: -6.668235896001967\n",
      "portentosa y  - neg. log-ll: -6.896892592016161\n",
      "colores la  - neg. log-ll: -6.682918735647495\n",
      "posibilidad de  - neg. log-ll: -6.667680377705841\n",
      "mujer ,  - neg. log-ll: -6.6687578055612535\n",
      "oro ,  - neg. log-ll: -6.672500521879665\n",
      "----------------------------------------------------------------------------------------------------\n",
      "buendia y y y y y y y y y y  - neg. log-ll: -6.9032502262336575\n",
      "niño de la aldea , y y y y y y  - neg. log-ll: -6.810183579819068\n",
      "épocas y y y y y y y y y y  - neg. log-ll: -6.90232485820038\n",
      "pasadas y y y y y y y y y y  - neg. log-ll: -6.903710657478321\n",
      "amaestrado y y y y y y y y y y  - neg. log-ll: -6.903354189063153\n",
      "artificios y y y y y y y y y y  - neg. log-ll: -6.879748793210489\n",
      "portentosa y y y y y y y y y y  - neg. log-ll: -6.902614462811909\n",
      "colores la aldea , y y y y y y y  - neg. log-ll: -6.833903169285874\n",
      "posibilidad de la aldea , y y y y y y  - neg. log-ll: -6.809016094073127\n",
      "mujer , y y y y y y y y y  - neg. log-ll: -6.879800984166417\n",
      "oro , y y y y y y y y y  - neg. log-ll: -6.8801752557982585\n"
     ]
    }
   ],
   "source": [
    "def pred_n_words(word = 'buendia',max_n_pred = 10):\n",
    "    full_pred = word\n",
    "    ll = 0\n",
    "    for i in range(max_n_pred):\n",
    "        word2 = predict_next_word(word)[0]\n",
    "        pr = predict_next_word(word)[1]\n",
    "        full_pred = full_pred + ' ' + word2\n",
    "        word = word2\n",
    "        ll += np.log(pr)\n",
    "    \n",
    "    n_ll = ll/max_n_pred\n",
    "\n",
    "    print(full_pred, ' - neg. log-ll:', n_ll)\n",
    "\n",
    "\n",
    "intersting_words = ['buendia','niño','épocas', 'pasadas','amaestrado','artificios',\n",
    "                    'portentosa','colores','posibilidad', 'mujer','oro']\n",
    "\n",
    "for w in intersting_words:\n",
    "    pred_n_words(word = w, max_n_pred=1)\n",
    "\n",
    "print(\"-\"*100)\n",
    "for w in intersting_words:\n",
    "    pred_n_words(word = w)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0e838c-3cab-4b6f-b041-1fde6a6d29aa",
   "metadata": {},
   "source": [
    "2. What does each value in the tensor represents?\n",
    "\n",
    "It depends, what is the tensor we want to analyse? The y vector represented as a tensor (the same as a simple np.array) has one number for each possible word of our bag of words, since we didn't perform further data cleaning such as lemmatization or Bag of Word reduction se have more than 6000 possible words. Thanks to sklearn we can easily go from the numeric vector to its word representation. The X matrix represented as a tensor is a one-hot encoding which means a dummy variable for each word of our Bag-of-Words. Intermediate tensors are standard neural network tensors, perhaps the most interesting in this example is the `torch.softmax(output, dim=1)`, where `output` represents the logit value, the final value of our NN before applying softmax, then  `torch.softmax` give us the predicted probabilty for each word of our BoW after a given input. It seems we should have cleaned stop words since the majority of our predicted words is \"y\". \n",
    "\n",
    "3. Why does it make sense to choose that number of neurons in our layer?\n",
    "\n",
    "Under sufficient data, we could argue that each possible output word needs its own model, hence each neuron could specialize in each word improving accuracy. For our case, we have a clear overfitting.\n",
    "\n",
    "4. What's the negative likelihood for each example?\n",
    "\n",
    "We have a -6.907 for \"pasadas\" as our lowest value and -6.672 for our highest value (\"oro\"). No initial word seems to show a reasonable prediction. Expected for a model with such low accuracy.\n",
    "\n",
    "\n",
    "5. Try generating a few sentences?\n",
    "\n",
    "Done!\n",
    "\n",
    "6. What's the negative likelihood for each sentence?\n",
    "\n",
    "We have a -6.9037 for the sentence generated from \"pasadas\" as our lowest value and -6.809 for our highest value (\"posibilidad\"). Ironically, the most complex sentence leads to the worst score, no sentence shows any meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e533cf-33b0-4a61-84cb-57c5793893ee",
   "metadata": {},
   "source": [
    "### Design your own neural network (more layers and different number of neurons)\n",
    "The goal is to get sentences that make more sense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d3136a66-c972-41be-83bd-9a573a44df4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/600], Train Loss: 7.6625, Train Accuracy: 0.02, Test Accuracy: 0.00\n",
      "Epoch [20/600], Train Loss: 7.6625, Train Accuracy: 0.02, Test Accuracy: 0.00\n",
      "Epoch [30/600], Train Loss: 7.6625, Train Accuracy: 0.64, Test Accuracy: 1.11\n",
      "Epoch [40/600], Train Loss: 7.6625, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [50/600], Train Loss: 7.6625, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [60/600], Train Loss: 7.6624, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [70/600], Train Loss: 7.6624, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [80/600], Train Loss: 7.6624, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [90/600], Train Loss: 7.6624, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [100/600], Train Loss: 7.6624, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [110/600], Train Loss: 7.6624, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [120/600], Train Loss: 7.6623, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [130/600], Train Loss: 7.6623, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [140/600], Train Loss: 7.6622, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [150/600], Train Loss: 7.6620, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [160/600], Train Loss: 7.6617, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [170/600], Train Loss: 7.6609, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [180/600], Train Loss: 7.6591, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [190/600], Train Loss: 7.6543, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [200/600], Train Loss: 7.6441, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [210/600], Train Loss: 7.6278, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [220/600], Train Loss: 7.6103, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [230/600], Train Loss: 7.6007, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [240/600], Train Loss: 7.5979, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [250/600], Train Loss: 7.5965, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [260/600], Train Loss: 7.5950, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [270/600], Train Loss: 7.5936, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [280/600], Train Loss: 7.5918, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [290/600], Train Loss: 7.5893, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [300/600], Train Loss: 7.5854, Train Accuracy: 6.36, Test Accuracy: 6.75\n",
      "Epoch [310/600], Train Loss: 7.5797, Train Accuracy: 12.40, Test Accuracy: 8.18\n",
      "Epoch [320/600], Train Loss: 7.5722, Train Accuracy: 12.46, Test Accuracy: 8.26\n",
      "Epoch [330/600], Train Loss: 7.5636, Train Accuracy: 12.48, Test Accuracy: 8.10\n",
      "Epoch [340/600], Train Loss: 7.5554, Train Accuracy: 12.48, Test Accuracy: 7.47\n",
      "Epoch [350/600], Train Loss: 7.5490, Train Accuracy: 12.48, Test Accuracy: 7.47\n",
      "Epoch [360/600], Train Loss: 7.5446, Train Accuracy: 12.48, Test Accuracy: 7.63\n",
      "Epoch [370/600], Train Loss: 7.5420, Train Accuracy: 12.48, Test Accuracy: 7.55\n",
      "Epoch [380/600], Train Loss: 7.5403, Train Accuracy: 12.48, Test Accuracy: 7.55\n",
      "Epoch [390/600], Train Loss: 7.5391, Train Accuracy: 12.48, Test Accuracy: 7.47\n",
      "Epoch [400/600], Train Loss: 7.5382, Train Accuracy: 12.48, Test Accuracy: 7.47\n",
      "Epoch [410/600], Train Loss: 7.5374, Train Accuracy: 12.48, Test Accuracy: 7.55\n",
      "Epoch [420/600], Train Loss: 7.5367, Train Accuracy: 12.48, Test Accuracy: 7.70\n",
      "Epoch [430/600], Train Loss: 7.5359, Train Accuracy: 12.48, Test Accuracy: 7.55\n",
      "Epoch [440/600], Train Loss: 7.5352, Train Accuracy: 12.48, Test Accuracy: 7.70\n",
      "Epoch [450/600], Train Loss: 7.5344, Train Accuracy: 12.48, Test Accuracy: 7.86\n",
      "Epoch [460/600], Train Loss: 7.5335, Train Accuracy: 12.48, Test Accuracy: 7.70\n",
      "Epoch [470/600], Train Loss: 7.5325, Train Accuracy: 12.48, Test Accuracy: 7.78\n",
      "Epoch [480/600], Train Loss: 7.5313, Train Accuracy: 12.48, Test Accuracy: 7.78\n",
      "Epoch [490/600], Train Loss: 7.5297, Train Accuracy: 14.09, Test Accuracy: 7.63\n",
      "Epoch [500/600], Train Loss: 7.5279, Train Accuracy: 14.19, Test Accuracy: 7.70\n",
      "Epoch [510/600], Train Loss: 7.5256, Train Accuracy: 14.19, Test Accuracy: 6.99\n",
      "Epoch [520/600], Train Loss: 7.5229, Train Accuracy: 15.24, Test Accuracy: 5.88\n",
      "Epoch [530/600], Train Loss: 7.5200, Train Accuracy: 15.24, Test Accuracy: 5.72\n",
      "Epoch [540/600], Train Loss: 7.5169, Train Accuracy: 15.24, Test Accuracy: 5.64\n",
      "Epoch [550/600], Train Loss: 7.5136, Train Accuracy: 17.39, Test Accuracy: 6.75\n",
      "Epoch [560/600], Train Loss: 7.5100, Train Accuracy: 17.86, Test Accuracy: 7.39\n",
      "Epoch [570/600], Train Loss: 7.5061, Train Accuracy: 17.48, Test Accuracy: 6.83\n",
      "Epoch [580/600], Train Loss: 7.5020, Train Accuracy: 17.48, Test Accuracy: 6.83\n",
      "Epoch [590/600], Train Loss: 7.4981, Train Accuracy: 17.48, Test Accuracy: 6.83\n",
      "Epoch [600/600], Train Loss: 7.4947, Train Accuracy: 17.48, Test Accuracy: 6.83\n"
     ]
    }
   ],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "\n",
    "# Define the model parameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size1 = 512\n",
    "hidden_size2 = 256\n",
    "output_size = len(label_encoder.classes_)\n",
    "\n",
    "model = SimpleNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 600 \n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = accuracy_score(y_train, predicted)\n",
    "\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        accuracy_test = accuracy_score(y_test, predicted)\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Train Accuracy: {accuracy * 100:.2f}, Test Accuracy: {accuracy_test * 100:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "914d0182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buendia la la la la la la la la la la  - neg. log-ll: -7.56879909073798\n",
      "niño , la la la la la la la la la  - neg. log-ll: -7.476387308631122\n",
      "épocas la la la la la la la la la la  - neg. log-ll: -7.572924327636565\n",
      "pasadas la la la la la la la la la la  - neg. log-ll: -7.57103997257893\n",
      "amaestrado la la la la la la la la la la  - neg. log-ll: -7.5700731546130084\n",
      "artificios y la la la la la la la la la  - neg. log-ll: -7.484689850515887\n",
      "portentosa la la la la la la la la la la  - neg. log-ll: -7.5728351144236585\n",
      "colores la la la la la la la la la la  - neg. log-ll: -7.537126639677136\n",
      "posibilidad de la la la la la la la la la  - neg. log-ll: -7.432913622062439\n",
      "mujer , la la la la la la la la la  - neg. log-ll: -7.476061086934763\n",
      "oro , la la la la la la la la la  - neg. log-ll: -7.476338530119925\n"
     ]
    }
   ],
   "source": [
    "intersting_words = ['buendia','niño','épocas', 'pasadas','amaestrado','artificios',\n",
    "                    'portentosa','colores','posibilidad', 'mujer','oro']\n",
    "\n",
    "for w in intersting_words:\n",
    "    pred_n_words(word = w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e713d4",
   "metadata": {},
   "source": [
    "After many tries we couldn't produce a model better than the initial NN. Consider that we are using a simple neural network, but we couldn't outperform a logistic regression. RNN are recommended for NLP problems.\n",
    "\n",
    "The majority of the code was produced using the asistance of ChatGPT, with some tailoring from the author to have more readable outputs. Log-likelihood code is from the class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
